{"cells":[{"cell_type":"markdown","source":["# Introdução ao Big Data com Apache Spark na Plataforma Databricks\n\nEste tutorial tem como objetivo facilitar o entendimento do conceito de Big Data utilizando o Apache Spark no ambiente da Databricks.\n\n## Terminologia da Databricks\n\nAntes de começar o tutorial, temos que explicar alguns conceitos da plataforma Databricks que serão base para o entendimento:\n\n-   ****Workspaces****\n    -   Workspaces permitem organizar todo o trabalho que será feito na Databricks. Como uma estrutura de pastas no computador, ele permite salvar os ****notebooks**** e ****libraries**** e compartilhar com outros usuários. Workspaces não são conectados com os dados e não deveriam ser utilizados para armazenar dados, mas para armazenar os ****notebooks**** e ****libraries**** que são utilizados para manipular os dados.\n    - Para mais detalhes acesse a documentação do [Workspace](https://docs.databricks.com/user-guide/workspace.html).\n-   ****Notebooks****\n    -   Notebooks são um conjunto de células utilizados para executar comandos. Células possuem códigos em qualquer uma das seguintes linguagens: `Scala`, `Python`, `R`, `SQL`, or `Markdown`. Notebooks possuem uma linguagem padrão, mas cada célula pode ter uma linguagem sobrescrevendo outra. Isto é feito incluindo `%[language name]` no topo da célula, por exemplo, `%python`.\n    -   Notebooks precisam estar conectados a um ****cluster**** para executarem comandos. Entretanto, eles não estão acoplados ao cluster o que permite que os notebooks possam ser compartilhados via web ou baixados na máquina local.\n    -   Aqui é possível ver mais detalhes na documentação dos [Notebooks](https://docs.databricks.com/user-guide/notebooks/index.html).\n    -   ****Dashboards****\n        -   ****Dashboards**** podem ser criados a partir dos ****notebooks**** como uma forma de mostrar a saída das células sem o código que as gera.\n    - ****Notebooks**** podem também ser escalonados como ****jobs**** em um clique para executar um pipeline de dados, atualizar um modelo de Machine Learning ou atualizar um dashboard.\n-   ****Libraries****\n    -   Libraries são pacotes ou módulos que adicionam funcionalidades que for preciso para resolver os problemas de negócio. Elas podem ser escritas em Scala ou Java (com jars), além de Python eggs ou pacotes customizados. As libraries podem ser enviadas de forma manual ou instaladas diretamente através de gerenciadores de pacotes como Pypi ou Maven.\n    - Você poderá encontrar mais detalhes aqui na documentação das [Libraries](https://docs.databricks.com/user-guide/libraries.html).\n-   ****Tables****\n    -   Tabelas são estruturas de dados utilizadas para análise. As tabelas podem existir em diversos lugares: armazenadas no Amazon S3, armazenadas no cluster ou estar na cache em memória. [Para saber mais sobre as tabelas, veja a documentação!](https://docs.databricks.com/user-guide/tables.html).\n-   ****Clusters****\n    -   Clusters são grupos de computadores que você pode tratar como um único servidor. Os Clusters permitem executar os códigos a partir dos ****notebooks**** ou ****libraries**** em um determinado conjunto de dados.\n    - É importante notar que os clusters possuem controle de acesso para que seja possível controlar o acesso dos usuários ao cluster.\n    -   Pra mais detalhes, veja a documentação da Databricks sobre os [Clusters](https://docs.databricks.com/user-guide/clusters/index.html#clusters).\n-   ****Jobs****\n    -   Jobs são a ferramenta pela qual você pode escalonar a execução dos códigos em um já existente ou em novo ****cluster****. Estes códigos podem ser ****notebooks**** como estarem dentro de jars ou scripts Python. Os Jobs podem ser criados manualmente ou via API REST.\n    -   Você pode ver aqui a documentação dos [Jobs](https://docs.databricks.com/user-guide/jobs.html).\n-   ****Apps****\n    -   Apps são integrações de aplicações terceiras com a plataforma Databricks. Incluem aplicações como Tableu.\n    \n## Documentação do Databricks e Apache Spark\n\nDatabricks possui uma variedade de ferramentas para auxiliar no aprendizado da plataforma Databricks e do Apache Spark. Para acessar a documentação clique no ícone de interrogação do canto superior direito da tela.\nEste menu de busca irá buscar nas seções de documentação abaixo.\n\n![img](https://docs.databricks.com/_images/help-menu.png)\n\n-   ****Documentation****\n    -   Guia de referência que permite uma rápida busca na API da Databricks e do Spark com pequenos trechos de códigos de exemplo.\n    -   O guia também inclui uma série de tutoriais introdutórios para cada tópico.\n-   ****Documentação do Apache Spark****\n    -   A documentação do Apache Spark também está disponível para uma busca rápida.\n-   ****REST API Docs****\n    -   Documentação da API REST que permite executar comandos diretamente na plataforma Databricks.\n-   ****Fórums do Databricks****\n    -   [Os fóruns do Databricks](https://forums.databricks.com/) permitem interagir com a comunidade através de perguntas específicas não englobadas pela documentação.\n    \n# Arquitetura do Apache Spark\n\nAntes de partir para o código, vamos ver uma visão geral da arquitetura do Apache Spark. Esta arquitetura permite que você possa processar seus códigos em várias máquinas como se fosse uma só através da arquitetura master-worker, onde existe um `driver` ou nó master no cluster, acompanhado pelos nós `worker`. O master envia o trabalho para os workers com instruções para carregar os dados da memória ou do disco.\n\nO diagrama abaixo apresenta um exemplo de um cluster com Apache Spark, onde basicamente existe um nó Driver que comunica com os nós executors. Cada um destes nós executors tem slots que são logicamente como núcleos de processsamento.\n\n![spark-architecture](http://training.databricks.com/databricks_guide/gentle_introduction/videoss_logo.png)\n\nO Driver envia Tasks para os slots vazios nos Executors quando o trabalho estiver terminado:\n\n![spark-architecture](http://training.databricks.com/databricks_guide/gentle_introduction/spark_cluster_tasks.png)\n\nNota: No caso da edição Community da plataforma Databricks, não existe Worker e o Master fica responsável por executar o código inteiro:\n\n![spark-architecture](http://training.databricks.com/databricks_guide/gentle_introduction/notebook_microcluster.png)\n\nVocê pode visualizar os detalhes da sua aplicação na interface web do Apache Spark acessível na plataforma Databricks clicando \"Clusters\" e então no link \"Spark UI\". O link também está disponível no canto superior esquerdo deste notebook onde você pode selecionar o cluster que está ligado (Attached) a este notebook."],"metadata":{}},{"cell_type":"markdown","source":["# Executando comandos Linux\n\nA plataforma permite que você execute comandos shell no seu notebook com a adição do comando %sh no início da célula. Você pode adicionar a opção *-e* para falhar a célula se o comando shell tiver um status de saída diferente de zero. Exemplos:\n\n%sh free -mh\n\n%sh -e free -~\n\n%sh ps -fe\n\n%sh ls -l /tmp"],"metadata":{}},{"cell_type":"code","source":["%sh free -mh"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">              total        used        free      shared  buff/cache   available\nMem:           5.9G        2.4G        3.3G         68M        141M        3.3G\nSwap:            9G          0B          9G\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["%sh ls -l /databricks"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">total 10060\n-r-xr-xr-x  1 root root      192 Jan  1  1970 BUILD\n-r-xr-xr-x  1 root root      304 Jan  1  1970 BUILDINFO\ndrwxr-xr-x  3 root root     4096 Sep 24 17:16 chauffeur\ndrwxr-xr-x  2 root root    20480 Jan  1  1970 chauffeur-jars\ndrwxr-xr-x  1 root root     4096 Jan  1  1970 common\ndrwxr-xr-x 13 root root     4096 Sep 20 17:53 conda\ndrwxr-xr-x  4 root root     4096 Sep 24 17:16 data\ndrwxr-xr-x  1 root root     4096 Sep 24 17:16 driver\ndrwxr-xr-x  3  502 root     4096 Jun 21  2016 ganglia-api\n-rw-r--r--  1 root root     9650 Jul  6  2017 ganglia-api-cbe773d051168e05118774708ff7a0ce881617f4.tar.gz\ndrwxr-xr-x  1 root root     4096 Sep 24 17:16 hive\ndrwxr-xr-x  2 root root     4096 Sep 24 17:16 init_scripts\ndrwxr-xr-x  2 root root    77824 Jan  1  1970 jars\ndrwxr-xr-x  2 root root     4096 Sep 24 17:16 keys\ndrwxr-xr-x  2 root root     4096 Jan  1  1970 licenses\ndrwxr-xr-x  2 root root     4096 Jan  1  1970 miniconda\nlrwxrwxrwx  1 root root       19 Sep 24 17:16 python -&gt; /databricks/python3\ndrwxr-xr-x  7 root root     4096 Sep 20 17:53 python2\ndrwxr-xr-x  1 root root     4096 Sep 20 17:53 python3\ndrwxr-xr-x  3 root root     4096 Jan  1  1970 python-bootstrap\n-rw-r--r--  1 root root 10120510 Mar  3  2015 simbasparkodbc_1.0.4-2_amd64.deb\ndrwxr-xr-x  1 root root     4096 Sep 20 17:53 spark\n</div>"]}}],"execution_count":4},{"cell_type":"markdown","source":["#Databricks File System - DBFS\n\n[Databricks File System](https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html) (DBFS) é um sistema de arquivos distribuído instalado no cluster da Databricks. Os dados são mantidos mesmo se o cluster for finalizado.\n\nVocê pode acessar os arquivos no DBFS usando o [Databricks CLI](https://docs.databricks.com/user-guide/dev-tools/databricks-cli.html#databricks-cli) e a [DBFS API](https://docs.databricks.com/api/latest/dbfs.html#dbfs-api) na máquina local. No cluster da Databricks, você pode utilizar o [Databricks Utilities](https://docs.databricks.com/user-guide/dev-tools/dbutils.html#dbutils), as APIs do Spark e a API local."],"metadata":{}},{"cell_type":"code","source":["# Acessando os datasets da Databricks com o DBUtils\ndisplay(dbutils.fs.ls(\"/databricks-datasets\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/databricks-datasets/README.md</td><td>README.md</td><td>976</td></tr><tr><td>dbfs:/databricks-datasets/Rdatasets/</td><td>Rdatasets/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/SPARK_README.md</td><td>SPARK_README.md</td><td>3359</td></tr><tr><td>dbfs:/databricks-datasets/adult/</td><td>adult/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/airlines/</td><td>airlines/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/amazon/</td><td>amazon/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/asa/</td><td>asa/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/atlas_higgs/</td><td>atlas_higgs/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/bikeSharing/</td><td>bikeSharing/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/cctvVideos/</td><td>cctvVideos/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/credit-card-fraud/</td><td>credit-card-fraud/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/cs100/</td><td>cs100/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/cs110x/</td><td>cs110x/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/cs190/</td><td>cs190/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/data.gov/</td><td>data.gov/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/definitive-guide/</td><td>definitive-guide/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/flights/</td><td>flights/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/genomics/</td><td>genomics/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/hail/</td><td>hail/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/iot/</td><td>iot/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/iot-stream/</td><td>iot-stream/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/learning-spark/</td><td>learning-spark/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/learning-spark-v2/</td><td>learning-spark-v2/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/lending-club-loan-stats/</td><td>lending-club-loan-stats/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/mnist-digits/</td><td>mnist-digits/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/news20.binary/</td><td>news20.binary/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/online_retail/</td><td>online_retail/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/overlap-join/</td><td>overlap-join/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/power-plant/</td><td>power-plant/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/sai-summit-2019-sf/</td><td>sai-summit-2019-sf/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/sample_logs/</td><td>sample_logs/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/samples/</td><td>samples/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/sfo_customer_survey/</td><td>sfo_customer_survey/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/sms_spam_collection/</td><td>sms_spam_collection/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/songs/</td><td>songs/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/</td><td>structured-streaming/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/tpch/</td><td>tpch/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/weather/</td><td>weather/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/wiki/</td><td>wiki/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/wikipedia-datasets/</td><td>wikipedia-datasets/</td><td>0</td></tr></tbody></table></div>"]}}],"execution_count":6},{"cell_type":"markdown","source":["As células possuem um atalho para acessar o sistema de arquivos utilizando o dbutils usando o comando mágico **%fs**:"],"metadata":{}},{"cell_type":"code","source":["%fs\nls /databricks-datasets"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/databricks-datasets/README.md</td><td>README.md</td><td>976</td></tr><tr><td>dbfs:/databricks-datasets/Rdatasets/</td><td>Rdatasets/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/SPARK_README.md</td><td>SPARK_README.md</td><td>3359</td></tr><tr><td>dbfs:/databricks-datasets/adult/</td><td>adult/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/airlines/</td><td>airlines/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/amazon/</td><td>amazon/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/asa/</td><td>asa/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/atlas_higgs/</td><td>atlas_higgs/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/bikeSharing/</td><td>bikeSharing/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/cctvVideos/</td><td>cctvVideos/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/credit-card-fraud/</td><td>credit-card-fraud/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/cs100/</td><td>cs100/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/cs110x/</td><td>cs110x/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/cs190/</td><td>cs190/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/data.gov/</td><td>data.gov/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/definitive-guide/</td><td>definitive-guide/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/flights/</td><td>flights/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/genomics/</td><td>genomics/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/hail/</td><td>hail/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/iot/</td><td>iot/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/iot-stream/</td><td>iot-stream/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/learning-spark/</td><td>learning-spark/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/learning-spark-v2/</td><td>learning-spark-v2/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/lending-club-loan-stats/</td><td>lending-club-loan-stats/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/mnist-digits/</td><td>mnist-digits/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/news20.binary/</td><td>news20.binary/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/online_retail/</td><td>online_retail/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/overlap-join/</td><td>overlap-join/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/power-plant/</td><td>power-plant/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/sai-summit-2019-sf/</td><td>sai-summit-2019-sf/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/sample_logs/</td><td>sample_logs/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/samples/</td><td>samples/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/sfo_customer_survey/</td><td>sfo_customer_survey/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/sms_spam_collection/</td><td>sms_spam_collection/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/songs/</td><td>songs/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/</td><td>structured-streaming/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/tpch/</td><td>tpch/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/weather/</td><td>weather/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/wiki/</td><td>wiki/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/wikipedia-datasets/</td><td>wikipedia-datasets/</td><td>0</td></tr></tbody></table></div>"]}}],"execution_count":8},{"cell_type":"markdown","source":["Você também pode utilizar as APIs locais do Python para acessar algum arquivo dentro da estrutura da Databricks. O caminho absoluto inicia com \"/dbfs\" e o código de leitura do arquivo README.md dentro do diretório de amostras de bases de dados da Databricks pode ser feito da seguinte forma:"],"metadata":{}},{"cell_type":"code","source":["%python\nwith open(\"/dbfs/databricks-datasets/samples/docs/README.md\") as f:\n    x = ''.join(f.readlines())\n\nprint(x)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Welcome to the Spark documentation!\n\nThis readme will walk you through navigating and building the Spark documentation, which is included\nhere with the Spark source code. You can also find documentation specific to release versions of\nSpark at http://spark.apache.org/documentation.html.\n\nRead on to learn more about viewing documentation in plain text (i.e., markdown) or building the\ndocumentation yourself. Why build it yourself? So that you have the docs that corresponds to\nwhichever version of Spark you currently have checked out of revision control.\n\n## Prerequisites\nThe Spark documentation build uses a number of tools to build HTML docs and API docs in Scala,\nPython and R. To get started you can run the following commands\n\n    $ sudo gem install jekyll\n    $ sudo gem install jekyll-redirect-from\n    $ sudo pip install Pygments\n    $ sudo pip install sphinx\n    $ Rscript -e &apos;install.packages(c(&quot;knitr&quot;, &quot;devtools&quot;), repos=&quot;http://cran.stat.ucla.edu/&quot;)&apos;\n\n\n## Generating the Documentation HTML\n\nWe include the Spark documentation as part of the source (as opposed to using a hosted wiki, such as\nthe github wiki, as the definitive documentation) to enable the documentation to evolve along with\nthe source code and be captured by revision control (currently git). This way the code automatically\nincludes the version of the documentation that is relevant regardless of which version or release\nyou have checked out or downloaded.\n\nIn this directory you will find textfiles formatted using Markdown, with an &quot;.md&quot; suffix. You can\nread those text files directly if you want. Start with index.md.\n\nExecute &#96;jekyll build&#96; from the &#96;docs/&#96; directory to compile the site. Compiling the site with\nJekyll will create a directory called &#96;_site&#96; containing index.html as well as the rest of the\ncompiled files.\n\n    $ cd docs\n    $ jekyll build\n\nYou can modify the default Jekyll build as follows:\n\n    # Skip generating API docs (which takes a while)\n    $ SKIP_API=1 jekyll build\n    # Serve content locally on port 4000\n    $ jekyll serve --watch\n    # Build the site with extra features used on the live page\n    $ PRODUCTION=1 jekyll build\n\n\n## API Docs (Scaladoc, Sphinx, roxygen2)\n\nYou can build just the Spark scaladoc by running &#96;build/sbt unidoc&#96; from the SPARK_PROJECT_ROOT directory.\n\nSimilarly, you can build just the PySpark docs by running &#96;make html&#96; from the\nSPARK_PROJECT_ROOT/python/docs directory. Documentation is only generated for classes that are listed as\npublic in &#96;__init__.py&#96;. The SparkR docs can be built by running SPARK_PROJECT_ROOT/R/create-docs.sh.\n\nWhen you run &#96;jekyll&#96; in the &#96;docs&#96; directory, it will also copy over the scaladoc for the various\nSpark subprojects into the &#96;docs&#96; directory (and then also into the &#96;_site&#96; directory). We use a\njekyll plugin to run &#96;build/sbt unidoc&#96; before building the site so if you haven&apos;t run it (recently) it\nmay take some time as it generates all of the scaladoc.  The jekyll plugin also generates the\nPySpark docs [Sphinx](http://sphinx-doc.org/).\n\nNOTE: To skip the step of building and copying over the Scala, Python, R API docs, run &#96;SKIP_API=1\njekyll&#96;.\n\n</div>"]}}],"execution_count":10},{"cell_type":"markdown","source":["É possível acessar os arquivos utilizando a API do Spark. O próximo comando lê o arquivo README.md e cria um DataFrame denominado *textFile*:"],"metadata":{}},{"cell_type":"code","source":["textFile = spark.read.text(\"/databricks-datasets/samples/docs/README.md\")\ntextFile.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+\n               value|\n+--------------------+\nWelcome to the Sp...|\n                    |\nThis readme will ...|\nhere with the Spa...|\nSpark at http://s...|\n                    |\nRead on to learn ...|\ndocumentation you...|\nwhichever version...|\n                    |\n    ## Prerequisites|\nThe Spark documen...|\nPython and R. To ...|\n                    |\n    $ sudo gem in...|\n    $ sudo gem in...|\n    $ sudo pip in...|\n    $ sudo pip in...|\n    $ Rscript -e ...|\n                    |\n+--------------------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["# Como utilizar o SparkSession - Um ponto de entrada unificado no Apache Spark 2.0\n\nNo Spark 2.0, foi introduzido o [SparkSession](https://spark.apache.org/docs/preview/api/python/pyspark.sql.html#pyspark.sql.SparkSession), um novo ponto de entrada que agrega o SparkContext, SQLContext, StreamingContext, and HiveContext. SparkSession possui muitas funcionalidades e nesse notebook vamos apresentar as mais importantes para ilustrar o acesso às funcionalidades do Spark. \n\nNa plataforma Databricks, o SparkSession é criado para você e armazenado na variável denominada **spark** que pode ser utilizada nos Notebooks."],"metadata":{}},{"cell_type":"code","source":["textFile = spark.read.text(\"/databricks-datasets/samples/docs/README.md\")\ntextFile.first()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">6</span><span class=\"ansired\">]: </span>Row(value=&apos;Welcome to the Spark documentation!&apos;)\n</div>"]}}],"execution_count":14},{"cell_type":"markdown","source":["**Q1**: Qual a versão do Spark utilizada neste notebook?"],"metadata":{}},{"cell_type":"code","source":["spark.version"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">7</span><span class=\"ansired\">]: </span>&apos;2.4.0&apos;\n</div>"]}}],"execution_count":16},{"cell_type":"code","source":["spark.sparkContext.version"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">8</span><span class=\"ansired\">]: </span>&apos;2.4.0&apos;\n</div>"]}}],"execution_count":17},{"cell_type":"markdown","source":["**Q2**: Quais os bancos de dados disponíveis no catálogo?"],"metadata":{}},{"cell_type":"code","source":["display(spark.catalog.listDatabases())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>description</th><th>locationUri</th></tr></thead><tbody><tr><td>default</td><td>Default Hive database</td><td>dbfs:/user/hive/warehouse</td></tr></tbody></table></div>"]}}],"execution_count":19},{"cell_type":"markdown","source":["## Criando Dataframes com Sparksession"],"metadata":{}},{"cell_type":"markdown","source":["O exemplo abaixo mostra a criação de um Dataframe de números de 20 até 100 com incremento de 10 unidades."],"metadata":{}},{"cell_type":"code","source":["spark.range(20, 100, 20).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+\n id|\n+---+\n 20|\n 40|\n 60|\n 80|\n+---+\n\n</div>"]}}],"execution_count":22},{"cell_type":"markdown","source":["O próximo exemplo mostra a criação de um Dataframe que infere o tipo de cada coluna e dá um nome serial para cada coluna começando com \\_1, depois \\_2 e assim sucessivamente. Veja na saída da célula que o tipo da primeira coluna foi inferido como String e da segunda coluna como Long."],"metadata":{}},{"cell_type":"code","source":["df = spark.createDataFrame([(\"Scala\", 15), (\"SQL\", 15), (\"Python\", 25), (\"R\", 5), (\"Java\", 40)])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":24},{"cell_type":"code","source":["display(df)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>_1</th><th>_2</th></tr></thead><tbody><tr><td>Scala</td><td>15</td></tr><tr><td>SQL</td><td>15</td></tr><tr><td>Python</td><td>25</td></tr><tr><td>R</td><td>5</td></tr><tr><td>Java</td><td>40</td></tr></tbody></table></div>"]}}],"execution_count":25},{"cell_type":"markdown","source":["Você também pode criar um Dataframe definindo o schema utilizando o [StructType](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.types.StructType) que é uma coleção de [StructFields](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.types.StructField). Cada StructField neste exemplo possui três parâmetros: 1) nome da coluna, 2) tipo da coluna e 3) se a coluna pode ser nula ou não."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import *\nschema = StructType([StructField(\"linguagem\", StringType(), True),\n                               StructField(\"porcentagem\", IntegerType(), True)])\ndf = spark.createDataFrame([(\"Scala\", 15), (\"SQL\", 15), (\"Python\", 25), (\"R\", 5), (\"Java\", 40), (None, 33)], schema)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":27},{"cell_type":"code","source":["display(df)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>linguagem</th><th>porcentagem</th></tr></thead><tbody><tr><td>Scala</td><td>15</td></tr><tr><td>SQL</td><td>15</td></tr><tr><td>Python</td><td>25</td></tr><tr><td>R</td><td>5</td></tr><tr><td>Java</td><td>40</td></tr><tr><td>null</td><td>33</td></tr></tbody></table></div>"]}}],"execution_count":28},{"cell_type":"markdown","source":["Você pode salvar o Dataframe como uma tabela e depois utilizar comandos Sql na manipulação desta tabela. As próximas duas células mostram um exemplo deste cenário."],"metadata":{}},{"cell_type":"code","source":["df.write.saveAsTable(\"linguagens\", mode='overwrite')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":30},{"cell_type":"code","source":["%sql select linguagem from linguagens where porcentagem < 25"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>linguagem</th></tr></thead><tbody><tr><td>Scala</td></tr><tr><td>SQL</td></tr><tr><td>R</td></tr></tbody></table></div>"]}}],"execution_count":31},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":32}],"metadata":{"name":"Introdução ao Big Data com Apache Spark na Plataforma Databricks","notebookId":3241280870852609},"nbformat":4,"nbformat_minor":0}
